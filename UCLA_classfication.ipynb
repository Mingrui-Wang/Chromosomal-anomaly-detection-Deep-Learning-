{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled",
      "provenance": [],
      "authorship_tag": "ABX9TyO2M1J+zW5QZezjjMxZ+iDG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mingrui-Wang/Inverse-of-a-Matrix/blob/master/UCLA_classfication.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p4j4bLuueDWY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 451
        },
        "outputId": "8e5dbc23-5e2b-4af8-c01e-114410809a5c"
      },
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "path = \"/content/drive/My Drive/暑期科研/\"\n",
        "\n",
        "os.chdir(path)\n",
        "os.listdir(path)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['9total_y.npy',\n",
              " '9total_x.npy',\n",
              " 'normal_hard.npy',\n",
              " 'normal_hard_label.npy',\n",
              " 'N_A_H_label.npy',\n",
              " 'N_A_H.npy',\n",
              " 'Anomaly_label.npy',\n",
              " 'Anomaly.npy',\n",
              " 'chromosome_9_label.npy',\n",
              " 'chromosome_9.npy',\n",
              " 'chromosome_harddata_label.npy',\n",
              " 'chromosome_harddata.npy',\n",
              " '测试集_label.npy',\n",
              " '测试集.npy',\n",
              " '训练集_label.npy',\n",
              " '训练集.npy',\n",
              " '训练集500_label.npy',\n",
              " '训练集500.npy',\n",
              " '训练集400.npy',\n",
              " '训练集400_label.npy',\n",
              " '训练集300_label.npy',\n",
              " '训练集300.npy',\n",
              " 'my_model.h5',\n",
              " 'student_data.csv']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9_sGWb0jd03s",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 355
        },
        "outputId": "6a0cff29-9c8b-415a-93fc-3b38d6247866"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "# Reading the csv file into a pandas DataFrame\n",
        "data = pd.read_csv('student_data.csv')\n",
        "# Printing out the first 10 rows of our data\n",
        "data[:10]"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>admit</th>\n",
              "      <th>gre</th>\n",
              "      <th>gpa</th>\n",
              "      <th>rank</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>380</td>\n",
              "      <td>3.61</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>660</td>\n",
              "      <td>3.67</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>800</td>\n",
              "      <td>4.00</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>640</td>\n",
              "      <td>3.19</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>520</td>\n",
              "      <td>2.93</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>1</td>\n",
              "      <td>760</td>\n",
              "      <td>3.00</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>1</td>\n",
              "      <td>560</td>\n",
              "      <td>2.98</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0</td>\n",
              "      <td>400</td>\n",
              "      <td>3.08</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>1</td>\n",
              "      <td>540</td>\n",
              "      <td>3.39</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0</td>\n",
              "      <td>700</td>\n",
              "      <td>3.92</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   admit  gre   gpa  rank\n",
              "0      0  380  3.61     3\n",
              "1      1  660  3.67     3\n",
              "2      1  800  4.00     1\n",
              "3      1  640  3.19     4\n",
              "4      0  520  2.93     4\n",
              "5      1  760  3.00     2\n",
              "6      1  560  2.98     1\n",
              "7      0  400  3.08     2\n",
              "8      1  540  3.39     3\n",
              "9      0  700  3.92     2"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "acwYq203eduY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Importing matplotlibimport matplotlib as mpl\\n\",\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "# Function to help us plot\n",
        "def plot_points(data):\n",
        "    X = np.array(data[['gre','gpa']])\n",
        "    y = np.array(data['admit'])\n",
        "    admitted = X[np.argwhere(y==1)]\n",
        "    rejected = X[np.argwhere(y==0)]\n",
        "    plt.scatter([s[0][0] for s in rejected], [s[0][1] for s in rejected], s = 25, color = 'red', edgecolor = 'k')\n",
        "    plt.scatter([s[0][0] for s in admitted], [s[0][1] for s in admitted], s = 25, color = 'cyan', edgecolor = 'k')\n",
        "    plt.xlabel('Test (GRE)')\n",
        "    plt.ylabel('Grades (GPA)')"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7a0cxaxfiBX9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 695
        },
        "outputId": "f5578f55-3561-4ba9-bd30-be298b1b360f"
      },
      "source": [
        "# Make dummy variables for rank\n",
        "one_hot_data = pd.concat([data, pd.get_dummies(data['rank'], prefix='rank')], axis=1)\n",
        "# Drop the previous rank column\n",
        "one_hot_data = one_hot_data.drop('rank', axis=1)\n",
        "\n",
        "# Print the first 10 rows of our data\n",
        "one_hot_data[:10]\n",
        "\n",
        "# Copying our data\\n\",\n",
        "processed_data = one_hot_data[:]\n",
        "\n",
        "# Scaling the columns\\n\",\n",
        "processed_data['gre'] = processed_data['gre']/800\n",
        "processed_data['gpa'] = processed_data['gpa']/4.0\n",
        "processed_data[:10]\n",
        "\n",
        "sample = np.random.choice(processed_data.index, size=int(len(processed_data)*0.8), replace=False)\n",
        "train_data, val_data = processed_data.iloc[sample], processed_data.drop(sample)\n",
        "\n",
        "print('Number of training samples is'+str(len(train_data)))\n",
        "print('Number of validation samples is'+str(len(val_data)))\n",
        "print(train_data[:10])\n",
        "\n",
        "print(val_data[:10])\n",
        "\n",
        "# Plotting the points\n",
        "plot_points(data)\n",
        "plt.show()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of training samples is320\n",
            "Number of validation samples is80\n",
            "     admit    gre     gpa  rank_1  rank_2  rank_3  rank_4\n",
            "152      1  0.800  0.9850       0       1       0       0\n",
            "244      0  0.675  0.7600       1       0       0       0\n",
            "388      0  0.800  0.7925       0       1       0       0\n",
            "300      0  0.800  0.8250       0       1       0       0\n",
            "392      1  0.750  0.8450       0       0       1       0\n",
            "85       0  0.650  0.7450       0       1       0       0\n",
            "231      0  0.750  0.8200       0       0       1       0\n",
            "291      0  1.000  0.6950       0       1       0       0\n",
            "283      0  0.650  0.7750       0       0       0       1\n",
            "58       0  0.500  0.9125       0       1       0       0\n",
            "    admit    gre     gpa  rank_1  rank_2  rank_3  rank_4\n",
            "0       0  0.475  0.9025       0       0       1       0\n",
            "7       0  0.500  0.7700       0       1       0       0\n",
            "8       1  0.675  0.8475       0       0       1       0\n",
            "11      0  0.550  0.8050       1       0       0       0\n",
            "12      1  0.950  1.0000       1       0       0       0\n",
            "13      0  0.875  0.7700       0       1       0       0\n",
            "15      0  0.600  0.8600       0       0       1       0\n",
            "22      0  0.750  0.7050       0       0       0       1\n",
            "26      1  0.775  0.9025       1       0       0       0\n",
            "33      1  1.000  1.0000       0       0       1       0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VPvpXgsnjE0N",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "99dfcf7e-2ea1-4c0b-d5aa-c5ed326bddd0"
      },
      "source": [
        "import keras\n",
        "\n",
        "# Separate data and one-hot encode the output\n",
        "# Note: We're also turning the data into numpy arrays, in order to train the model in Keras\n",
        "features = np.array(train_data.drop('admit', axis=1))\n",
        "targets = np.array(keras.utils.to_categorical(train_data['admit'], 2))\n",
        "features_val = np.array(val_data.drop('admit', axis=1))\n",
        "targets_val = np.array(keras.utils.to_categorical(val_data['admit'], 2))\n",
        "# Imports\n",
        "import numpy as np\n",
        "from keras.models import Sequential\n",
        "from keras.layers.core import Dense, Dropout, Activation\n",
        "from keras.optimizers import SGD\n",
        "from keras.utils import np_utils\n",
        "\n",
        "from keras.layers import Input, Dense, Dropout\n",
        "from keras.models import Model\n",
        "\n",
        "# Input tensor\n",
        "inputs = Input(shape=(6,))\n",
        "# TODO: Building the model\n",
        "x = Dense(6, activation='relu')(inputs)\n",
        "x = Dense(64, activation='relu')(x)\n",
        "predictions = Dense(2, activation='softmax')(x)\n",
        "\n",
        "# This creates a model that includes\n",
        "# the Input layer and three Dense layers\n",
        "model = Model(inputs=inputs, outputs=predictions)\n",
        "\n",
        "# Compiling the model\n",
        "model.compile(loss = 'categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"functional_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, 6)]               0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 6)                 42        \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 64)                448       \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 2)                 130       \n",
            "=================================================================\n",
            "Total params: 620\n",
            "Trainable params: 620\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F61ry4eIj6gF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "04e52c98-e8e0-41d9-daa0-812e899f683b"
      },
      "source": [
        "history = model.fit(features, targets, batch_size=10,\n",
        "                                  epochs=200,\n",
        "                                  validation_data=(features_val, targets_val))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.6432 - accuracy: 0.6750 - val_loss: 0.6072 - val_accuracy: 0.7125\n",
            "Epoch 2/200\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.6287 - accuracy: 0.6750 - val_loss: 0.5949 - val_accuracy: 0.7125\n",
            "Epoch 3/200\n",
            "32/32 [==============================] - 0s 1ms/step - loss: 0.6243 - accuracy: 0.6750 - val_loss: 0.5951 - val_accuracy: 0.7125\n",
            "Epoch 4/200\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.6236 - accuracy: 0.6750 - val_loss: 0.5938 - val_accuracy: 0.7125\n",
            "Epoch 5/200\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.6207 - accuracy: 0.6750 - val_loss: 0.5920 - val_accuracy: 0.7125\n",
            "Epoch 6/200\n",
            "32/32 [==============================] - 0s 1ms/step - loss: 0.6193 - accuracy: 0.6750 - val_loss: 0.5907 - val_accuracy: 0.7125\n",
            "Epoch 7/200\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.6192 - accuracy: 0.6750 - val_loss: 0.5913 - val_accuracy: 0.7125\n",
            "Epoch 8/200\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.6170 - accuracy: 0.6750 - val_loss: 0.5914 - val_accuracy: 0.7125\n",
            "Epoch 9/200\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.6155 - accuracy: 0.6750 - val_loss: 0.5882 - val_accuracy: 0.7125\n",
            "Epoch 10/200\n",
            "32/32 [==============================] - 0s 1ms/step - loss: 0.6136 - accuracy: 0.6750 - val_loss: 0.5886 - val_accuracy: 0.7125\n",
            "Epoch 11/200\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.6126 - accuracy: 0.6750 - val_loss: 0.5892 - val_accuracy: 0.7125\n",
            "Epoch 12/200\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.6115 - accuracy: 0.6750 - val_loss: 0.5892 - val_accuracy: 0.7125\n",
            "Epoch 13/200\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.6100 - accuracy: 0.6750 - val_loss: 0.5909 - val_accuracy: 0.7125\n",
            "Epoch 14/200\n",
            "32/32 [==============================] - 0s 1ms/step - loss: 0.6089 - accuracy: 0.6875 - val_loss: 0.5905 - val_accuracy: 0.7125\n",
            "Epoch 15/200\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.6059 - accuracy: 0.6750 - val_loss: 0.5920 - val_accuracy: 0.7125\n",
            "Epoch 16/200\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.6047 - accuracy: 0.6656 - val_loss: 0.5932 - val_accuracy: 0.7500\n",
            "Epoch 17/200\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.6028 - accuracy: 0.6938 - val_loss: 0.5949 - val_accuracy: 0.7750\n",
            "Epoch 18/200\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.6017 - accuracy: 0.6844 - val_loss: 0.5971 - val_accuracy: 0.7625\n",
            "Epoch 19/200\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.6004 - accuracy: 0.6938 - val_loss: 0.5997 - val_accuracy: 0.7750\n",
            "Epoch 20/200\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.5982 - accuracy: 0.6875 - val_loss: 0.5998 - val_accuracy: 0.7750\n",
            "Epoch 21/200\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.5982 - accuracy: 0.6844 - val_loss: 0.6015 - val_accuracy: 0.7750\n",
            "Epoch 22/200\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.5980 - accuracy: 0.6844 - val_loss: 0.6054 - val_accuracy: 0.7750\n",
            "Epoch 23/200\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.5963 - accuracy: 0.6906 - val_loss: 0.6024 - val_accuracy: 0.7750\n",
            "Epoch 24/200\n",
            "32/32 [==============================] - 0s 1ms/step - loss: 0.5944 - accuracy: 0.6906 - val_loss: 0.6091 - val_accuracy: 0.7750\n",
            "Epoch 25/200\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.5930 - accuracy: 0.6938 - val_loss: 0.6073 - val_accuracy: 0.7750\n",
            "Epoch 26/200\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.5947 - accuracy: 0.6844 - val_loss: 0.6100 - val_accuracy: 0.7750\n",
            "Epoch 27/200\n",
            "32/32 [==============================] - 0s 1ms/step - loss: 0.5918 - accuracy: 0.6969 - val_loss: 0.6106 - val_accuracy: 0.7750\n",
            "Epoch 28/200\n",
            "32/32 [==============================] - 0s 1ms/step - loss: 0.5922 - accuracy: 0.6812 - val_loss: 0.6108 - val_accuracy: 0.7750\n",
            "Epoch 29/200\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.5898 - accuracy: 0.6969 - val_loss: 0.6128 - val_accuracy: 0.7750\n",
            "Epoch 30/200\n",
            "32/32 [==============================] - 0s 1ms/step - loss: 0.5899 - accuracy: 0.6875 - val_loss: 0.6153 - val_accuracy: 0.7750\n",
            "Epoch 31/200\n",
            "32/32 [==============================] - 0s 1ms/step - loss: 0.5912 - accuracy: 0.6844 - val_loss: 0.6154 - val_accuracy: 0.7625\n",
            "Epoch 32/200\n",
            "32/32 [==============================] - 0s 1ms/step - loss: 0.5896 - accuracy: 0.6969 - val_loss: 0.6156 - val_accuracy: 0.7750\n",
            "Epoch 33/200\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.5889 - accuracy: 0.6938 - val_loss: 0.6171 - val_accuracy: 0.7750\n",
            "Epoch 34/200\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.5896 - accuracy: 0.6906 - val_loss: 0.6196 - val_accuracy: 0.7750\n",
            "Epoch 35/200\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.5901 - accuracy: 0.6906 - val_loss: 0.6191 - val_accuracy: 0.7625\n",
            "Epoch 36/200\n",
            "32/32 [==============================] - 0s 1ms/step - loss: 0.5887 - accuracy: 0.6844 - val_loss: 0.6143 - val_accuracy: 0.7750\n",
            "Epoch 37/200\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.5892 - accuracy: 0.6906 - val_loss: 0.6180 - val_accuracy: 0.7750\n",
            "Epoch 38/200\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.5885 - accuracy: 0.6906 - val_loss: 0.6176 - val_accuracy: 0.7750\n",
            "Epoch 39/200\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.5878 - accuracy: 0.6938 - val_loss: 0.6188 - val_accuracy: 0.7750\n",
            "Epoch 40/200\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.5873 - accuracy: 0.6812 - val_loss: 0.6178 - val_accuracy: 0.7625\n",
            "Epoch 41/200\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.5880 - accuracy: 0.6938 - val_loss: 0.6210 - val_accuracy: 0.7625\n",
            "Epoch 42/200\n",
            "32/32 [==============================] - 0s 1ms/step - loss: 0.5883 - accuracy: 0.6844 - val_loss: 0.6193 - val_accuracy: 0.7625\n",
            "Epoch 43/200\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.5880 - accuracy: 0.6906 - val_loss: 0.6163 - val_accuracy: 0.7750\n",
            "Epoch 44/200\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.5874 - accuracy: 0.6875 - val_loss: 0.6156 - val_accuracy: 0.7750\n",
            "Epoch 45/200\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.5866 - accuracy: 0.7000 - val_loss: 0.6201 - val_accuracy: 0.7625\n",
            "Epoch 46/200\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.5903 - accuracy: 0.6906 - val_loss: 0.6217 - val_accuracy: 0.7625\n",
            "Epoch 47/200\n",
            "32/32 [==============================] - 0s 1ms/step - loss: 0.5854 - accuracy: 0.6938 - val_loss: 0.6182 - val_accuracy: 0.7625\n",
            "Epoch 48/200\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.5878 - accuracy: 0.6938 - val_loss: 0.6197 - val_accuracy: 0.7625\n",
            "Epoch 49/200\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.5859 - accuracy: 0.6969 - val_loss: 0.6135 - val_accuracy: 0.7750\n",
            "Epoch 50/200\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.5857 - accuracy: 0.6938 - val_loss: 0.6163 - val_accuracy: 0.7625\n",
            "Epoch 51/200\n",
            "32/32 [==============================] - 0s 1ms/step - loss: 0.5841 - accuracy: 0.6938 - val_loss: 0.6155 - val_accuracy: 0.7625\n",
            "Epoch 52/200\n",
            "32/32 [==============================] - 0s 1ms/step - loss: 0.5858 - accuracy: 0.6875 - val_loss: 0.6151 - val_accuracy: 0.7625\n",
            "Epoch 53/200\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.5845 - accuracy: 0.6906 - val_loss: 0.6174 - val_accuracy: 0.7625\n",
            "Epoch 54/200\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.5839 - accuracy: 0.6906 - val_loss: 0.6138 - val_accuracy: 0.7625\n",
            "Epoch 55/200\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.5839 - accuracy: 0.6969 - val_loss: 0.6155 - val_accuracy: 0.7625\n",
            "Epoch 56/200\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.5855 - accuracy: 0.6906 - val_loss: 0.6172 - val_accuracy: 0.7625\n",
            "Epoch 57/200\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.5850 - accuracy: 0.6969 - val_loss: 0.6169 - val_accuracy: 0.7625\n",
            "Epoch 58/200\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.5856 - accuracy: 0.7000 - val_loss: 0.6132 - val_accuracy: 0.7625\n",
            "Epoch 59/200\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.5843 - accuracy: 0.6938 - val_loss: 0.6119 - val_accuracy: 0.7625\n",
            "Epoch 60/200\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.5846 - accuracy: 0.7000 - val_loss: 0.6141 - val_accuracy: 0.7625\n",
            "Epoch 61/200\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.5848 - accuracy: 0.6906 - val_loss: 0.6125 - val_accuracy: 0.7625\n",
            "Epoch 62/200\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.5826 - accuracy: 0.6938 - val_loss: 0.6135 - val_accuracy: 0.7500\n",
            "Epoch 63/200\n",
            "32/32 [==============================] - 0s 1ms/step - loss: 0.5835 - accuracy: 0.6875 - val_loss: 0.6113 - val_accuracy: 0.7625\n",
            "Epoch 64/200\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.5823 - accuracy: 0.6938 - val_loss: 0.6103 - val_accuracy: 0.7625\n",
            "Epoch 65/200\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.5851 - accuracy: 0.6906 - val_loss: 0.6117 - val_accuracy: 0.7625\n",
            "Epoch 66/200\n",
            "32/32 [==============================] - 0s 1ms/step - loss: 0.5833 - accuracy: 0.6938 - val_loss: 0.6096 - val_accuracy: 0.7625\n",
            "Epoch 67/200\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.5838 - accuracy: 0.6750 - val_loss: 0.6122 - val_accuracy: 0.7625\n",
            "Epoch 68/200\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.5820 - accuracy: 0.6938 - val_loss: 0.6089 - val_accuracy: 0.7625\n",
            "Epoch 69/200\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.5824 - accuracy: 0.6938 - val_loss: 0.6093 - val_accuracy: 0.7625\n",
            "Epoch 70/200\n",
            "32/32 [==============================] - 0s 1ms/step - loss: 0.5833 - accuracy: 0.6969 - val_loss: 0.6128 - val_accuracy: 0.7625\n",
            "Epoch 71/200\n",
            "32/32 [==============================] - 0s 1ms/step - loss: 0.5817 - accuracy: 0.6906 - val_loss: 0.6102 - val_accuracy: 0.7625\n",
            "Epoch 72/200\n",
            "32/32 [==============================] - 0s 1ms/step - loss: 0.5823 - accuracy: 0.6906 - val_loss: 0.6097 - val_accuracy: 0.7625\n",
            "Epoch 73/200\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.5816 - accuracy: 0.6938 - val_loss: 0.6071 - val_accuracy: 0.7750\n",
            "Epoch 74/200\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.5891 - accuracy: 0.6844 - val_loss: 0.6083 - val_accuracy: 0.7625\n",
            "Epoch 75/200\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.5814 - accuracy: 0.6938 - val_loss: 0.6066 - val_accuracy: 0.7625\n",
            "Epoch 76/200\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.5838 - accuracy: 0.6969 - val_loss: 0.6093 - val_accuracy: 0.7625\n",
            "Epoch 77/200\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.5840 - accuracy: 0.7000 - val_loss: 0.6042 - val_accuracy: 0.7750\n",
            "Epoch 78/200\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.5801 - accuracy: 0.6969 - val_loss: 0.6116 - val_accuracy: 0.7625\n",
            "Epoch 79/200\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.5811 - accuracy: 0.6875 - val_loss: 0.6102 - val_accuracy: 0.7625\n",
            "Epoch 80/200\n",
            "32/32 [==============================] - 0s 1ms/step - loss: 0.5835 - accuracy: 0.7031 - val_loss: 0.6072 - val_accuracy: 0.7625\n",
            "Epoch 81/200\n",
            "32/32 [==============================] - 0s 1ms/step - loss: 0.5828 - accuracy: 0.7000 - val_loss: 0.6057 - val_accuracy: 0.7625\n",
            "Epoch 82/200\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.5810 - accuracy: 0.6938 - val_loss: 0.6048 - val_accuracy: 0.7625\n",
            "Epoch 83/200\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.5828 - accuracy: 0.6938 - val_loss: 0.6049 - val_accuracy: 0.7625\n",
            "Epoch 84/200\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.5827 - accuracy: 0.7000 - val_loss: 0.6088 - val_accuracy: 0.7625\n",
            "Epoch 85/200\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.5797 - accuracy: 0.6938 - val_loss: 0.6043 - val_accuracy: 0.7625\n",
            "Epoch 86/200\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.5833 - accuracy: 0.6938 - val_loss: 0.6075 - val_accuracy: 0.7625\n",
            "Epoch 87/200\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.5797 - accuracy: 0.6906 - val_loss: 0.6053 - val_accuracy: 0.7625\n",
            "Epoch 88/200\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.5811 - accuracy: 0.6969 - val_loss: 0.6064 - val_accuracy: 0.7625\n",
            "Epoch 89/200\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.5800 - accuracy: 0.6969 - val_loss: 0.6047 - val_accuracy: 0.7625\n",
            "Epoch 90/200\n",
            "32/32 [==============================] - 0s 1ms/step - loss: 0.5811 - accuracy: 0.6969 - val_loss: 0.6053 - val_accuracy: 0.7625\n",
            "Epoch 91/200\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.5818 - accuracy: 0.6906 - val_loss: 0.6027 - val_accuracy: 0.7625\n",
            "Epoch 92/200\n",
            "32/32 [==============================] - 0s 1ms/step - loss: 0.5803 - accuracy: 0.6938 - val_loss: 0.6045 - val_accuracy: 0.7625\n",
            "Epoch 93/200\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.5806 - accuracy: 0.6969 - val_loss: 0.6029 - val_accuracy: 0.7625\n",
            "Epoch 94/200\n",
            "32/32 [==============================] - 0s 1ms/step - loss: 0.5802 - accuracy: 0.6969 - val_loss: 0.6017 - val_accuracy: 0.7625\n",
            "Epoch 95/200\n",
            "32/32 [==============================] - 0s 1ms/step - loss: 0.5829 - accuracy: 0.6906 - val_loss: 0.6027 - val_accuracy: 0.7625\n",
            "Epoch 96/200\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.5924 - accuracy: 0.6750 - val_loss: 0.6020 - val_accuracy: 0.7625\n",
            "Epoch 97/200\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.5802 - accuracy: 0.6938 - val_loss: 0.6013 - val_accuracy: 0.7625\n",
            "Epoch 98/200\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.5795 - accuracy: 0.6906 - val_loss: 0.6058 - val_accuracy: 0.7625\n",
            "Epoch 99/200\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.5791 - accuracy: 0.6938 - val_loss: 0.6033 - val_accuracy: 0.7625\n",
            "Epoch 100/200\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.5793 - accuracy: 0.6906 - val_loss: 0.6029 - val_accuracy: 0.7625\n",
            "Epoch 101/200\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.5800 - accuracy: 0.6938 - val_loss: 0.6030 - val_accuracy: 0.7625\n",
            "Epoch 102/200\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.5794 - accuracy: 0.6938 - val_loss: 0.5988 - val_accuracy: 0.7625\n",
            "Epoch 103/200\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.5788 - accuracy: 0.7031 - val_loss: 0.6016 - val_accuracy: 0.7625\n",
            "Epoch 104/200\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.5809 - accuracy: 0.6906 - val_loss: 0.5993 - val_accuracy: 0.7625\n",
            "Epoch 105/200\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.5808 - accuracy: 0.6938 - val_loss: 0.6015 - val_accuracy: 0.7625\n",
            "Epoch 106/200\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.5793 - accuracy: 0.6938 - val_loss: 0.6023 - val_accuracy: 0.7625\n",
            "Epoch 107/200\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.5794 - accuracy: 0.6938 - val_loss: 0.5988 - val_accuracy: 0.7625\n",
            "Epoch 108/200\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.5792 - accuracy: 0.6969 - val_loss: 0.6037 - val_accuracy: 0.7625\n",
            "Epoch 109/200\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.5790 - accuracy: 0.6906 - val_loss: 0.6014 - val_accuracy: 0.7625\n",
            "Epoch 110/200\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.5786 - accuracy: 0.6938 - val_loss: 0.5988 - val_accuracy: 0.7625\n",
            "Epoch 111/200\n",
            "32/32 [==============================] - 0s 1ms/step - loss: 0.5814 - accuracy: 0.7000 - val_loss: 0.5998 - val_accuracy: 0.7625\n",
            "Epoch 112/200\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.5805 - accuracy: 0.6938 - val_loss: 0.5987 - val_accuracy: 0.7750\n",
            "Epoch 113/200\n",
            "32/32 [==============================] - 0s 1ms/step - loss: 0.5780 - accuracy: 0.6938 - val_loss: 0.6011 - val_accuracy: 0.7625\n",
            "Epoch 114/200\n",
            "32/32 [==============================] - 0s 1ms/step - loss: 0.5790 - accuracy: 0.7000 - val_loss: 0.6000 - val_accuracy: 0.7625\n",
            "Epoch 115/200\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.5786 - accuracy: 0.6969 - val_loss: 0.6004 - val_accuracy: 0.7625\n",
            "Epoch 116/200\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.5801 - accuracy: 0.6938 - val_loss: 0.5980 - val_accuracy: 0.7625\n",
            "Epoch 117/200\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.5780 - accuracy: 0.6906 - val_loss: 0.5972 - val_accuracy: 0.7625\n",
            "Epoch 118/200\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.5787 - accuracy: 0.6938 - val_loss: 0.5977 - val_accuracy: 0.7625\n",
            "Epoch 119/200\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.5783 - accuracy: 0.6938 - val_loss: 0.5985 - val_accuracy: 0.7625\n",
            "Epoch 120/200\n",
            "32/32 [==============================] - 0s 1ms/step - loss: 0.5787 - accuracy: 0.6906 - val_loss: 0.6005 - val_accuracy: 0.7625\n",
            "Epoch 121/200\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.5798 - accuracy: 0.7000 - val_loss: 0.5952 - val_accuracy: 0.7625\n",
            "Epoch 122/200\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.5784 - accuracy: 0.7031 - val_loss: 0.6014 - val_accuracy: 0.7625\n",
            "Epoch 123/200\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.5782 - accuracy: 0.6969 - val_loss: 0.5997 - val_accuracy: 0.7625\n",
            "Epoch 124/200\n",
            "32/32 [==============================] - 0s 1ms/step - loss: 0.5794 - accuracy: 0.7031 - val_loss: 0.5964 - val_accuracy: 0.7625\n",
            "Epoch 125/200\n",
            "32/32 [==============================] - 0s 1ms/step - loss: 0.5787 - accuracy: 0.6875 - val_loss: 0.5986 - val_accuracy: 0.7625\n",
            "Epoch 126/200\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.5775 - accuracy: 0.6906 - val_loss: 0.5961 - val_accuracy: 0.7625\n",
            "Epoch 127/200\n",
            "32/32 [==============================] - 0s 1ms/step - loss: 0.5791 - accuracy: 0.6969 - val_loss: 0.5959 - val_accuracy: 0.7625\n",
            "Epoch 128/200\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.5780 - accuracy: 0.6938 - val_loss: 0.5981 - val_accuracy: 0.7625\n",
            "Epoch 129/200\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.5790 - accuracy: 0.7031 - val_loss: 0.5975 - val_accuracy: 0.7625\n",
            "Epoch 130/200\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.5789 - accuracy: 0.6969 - val_loss: 0.5964 - val_accuracy: 0.7625\n",
            "Epoch 131/200\n",
            "32/32 [==============================] - 0s 1ms/step - loss: 0.5786 - accuracy: 0.6844 - val_loss: 0.5975 - val_accuracy: 0.7625\n",
            "Epoch 132/200\n",
            "32/32 [==============================] - 0s 1ms/step - loss: 0.5776 - accuracy: 0.6906 - val_loss: 0.5980 - val_accuracy: 0.7625\n",
            "Epoch 133/200\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.5777 - accuracy: 0.6969 - val_loss: 0.5952 - val_accuracy: 0.7625\n",
            "Epoch 134/200\n",
            "32/32 [==============================] - 0s 1ms/step - loss: 0.5807 - accuracy: 0.6906 - val_loss: 0.5990 - val_accuracy: 0.7625\n",
            "Epoch 135/200\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.5759 - accuracy: 0.6844 - val_loss: 0.5986 - val_accuracy: 0.7250\n",
            "Epoch 136/200\n",
            "32/32 [==============================] - 0s 1ms/step - loss: 0.5761 - accuracy: 0.7031 - val_loss: 0.5958 - val_accuracy: 0.7625\n",
            "Epoch 137/200\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.5769 - accuracy: 0.6969 - val_loss: 0.5947 - val_accuracy: 0.7625\n",
            "Epoch 138/200\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.5771 - accuracy: 0.6969 - val_loss: 0.5986 - val_accuracy: 0.7250\n",
            "Epoch 139/200\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.5776 - accuracy: 0.6938 - val_loss: 0.5951 - val_accuracy: 0.7625\n",
            "Epoch 140/200\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.5785 - accuracy: 0.6906 - val_loss: 0.5977 - val_accuracy: 0.7375\n",
            "Epoch 141/200\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.5782 - accuracy: 0.6969 - val_loss: 0.5970 - val_accuracy: 0.7625\n",
            "Epoch 142/200\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.5771 - accuracy: 0.6906 - val_loss: 0.5943 - val_accuracy: 0.7625\n",
            "Epoch 143/200\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.5795 - accuracy: 0.6906 - val_loss: 0.5921 - val_accuracy: 0.7625\n",
            "Epoch 144/200\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.5766 - accuracy: 0.6969 - val_loss: 0.5966 - val_accuracy: 0.7625\n",
            "Epoch 145/200\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.5764 - accuracy: 0.7094 - val_loss: 0.5967 - val_accuracy: 0.7250\n",
            "Epoch 146/200\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.5794 - accuracy: 0.6969 - val_loss: 0.5912 - val_accuracy: 0.7625\n",
            "Epoch 147/200\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.5790 - accuracy: 0.7031 - val_loss: 0.5953 - val_accuracy: 0.7375\n",
            "Epoch 148/200\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.5764 - accuracy: 0.7063 - val_loss: 0.5939 - val_accuracy: 0.7625\n",
            "Epoch 149/200\n",
            "32/32 [==============================] - 0s 1ms/step - loss: 0.5785 - accuracy: 0.6938 - val_loss: 0.5940 - val_accuracy: 0.7625\n",
            "Epoch 150/200\n",
            "32/32 [==============================] - 0s 1ms/step - loss: 0.5760 - accuracy: 0.6875 - val_loss: 0.5937 - val_accuracy: 0.7625\n",
            "Epoch 151/200\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.5768 - accuracy: 0.7000 - val_loss: 0.5937 - val_accuracy: 0.7625\n",
            "Epoch 152/200\n",
            "32/32 [==============================] - 0s 1ms/step - loss: 0.5786 - accuracy: 0.6969 - val_loss: 0.5918 - val_accuracy: 0.7750\n",
            "Epoch 153/200\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.5804 - accuracy: 0.6906 - val_loss: 0.6004 - val_accuracy: 0.7125\n",
            "Epoch 154/200\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.5794 - accuracy: 0.7031 - val_loss: 0.5913 - val_accuracy: 0.7625\n",
            "Epoch 155/200\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.5759 - accuracy: 0.7000 - val_loss: 0.5946 - val_accuracy: 0.7500\n",
            "Epoch 156/200\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.5770 - accuracy: 0.7125 - val_loss: 0.5941 - val_accuracy: 0.7625\n",
            "Epoch 157/200\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.5803 - accuracy: 0.6906 - val_loss: 0.5923 - val_accuracy: 0.7625\n",
            "Epoch 158/200\n",
            "32/32 [==============================] - 0s 1ms/step - loss: 0.5767 - accuracy: 0.6844 - val_loss: 0.5947 - val_accuracy: 0.7250\n",
            "Epoch 159/200\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.5760 - accuracy: 0.7031 - val_loss: 0.5913 - val_accuracy: 0.7625\n",
            "Epoch 160/200\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.5761 - accuracy: 0.6969 - val_loss: 0.5945 - val_accuracy: 0.7250\n",
            "Epoch 161/200\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.5764 - accuracy: 0.6938 - val_loss: 0.5939 - val_accuracy: 0.7250\n",
            "Epoch 162/200\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.5752 - accuracy: 0.7063 - val_loss: 0.5927 - val_accuracy: 0.7625\n",
            "Epoch 163/200\n",
            "32/32 [==============================] - 0s 1ms/step - loss: 0.5807 - accuracy: 0.6938 - val_loss: 0.5934 - val_accuracy: 0.7250\n",
            "Epoch 164/200\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.5766 - accuracy: 0.6938 - val_loss: 0.5902 - val_accuracy: 0.7625\n",
            "Epoch 165/200\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.5757 - accuracy: 0.6969 - val_loss: 0.5933 - val_accuracy: 0.7250\n",
            "Epoch 166/200\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.5752 - accuracy: 0.7000 - val_loss: 0.5921 - val_accuracy: 0.7500\n",
            "Epoch 167/200\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.5756 - accuracy: 0.7156 - val_loss: 0.5927 - val_accuracy: 0.7625\n",
            "Epoch 168/200\n",
            "32/32 [==============================] - 0s 1ms/step - loss: 0.5753 - accuracy: 0.7000 - val_loss: 0.5924 - val_accuracy: 0.7250\n",
            "Epoch 169/200\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.5761 - accuracy: 0.7063 - val_loss: 0.5897 - val_accuracy: 0.7625\n",
            "Epoch 170/200\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.5764 - accuracy: 0.6938 - val_loss: 0.5896 - val_accuracy: 0.7625\n",
            "Epoch 171/200\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.5741 - accuracy: 0.6938 - val_loss: 0.5953 - val_accuracy: 0.7250\n",
            "Epoch 172/200\n",
            "32/32 [==============================] - 0s 1ms/step - loss: 0.5768 - accuracy: 0.7031 - val_loss: 0.5935 - val_accuracy: 0.7375\n",
            "Epoch 173/200\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.5769 - accuracy: 0.7156 - val_loss: 0.5951 - val_accuracy: 0.7250\n",
            "Epoch 174/200\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.5756 - accuracy: 0.6969 - val_loss: 0.5904 - val_accuracy: 0.7625\n",
            "Epoch 175/200\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.5764 - accuracy: 0.7094 - val_loss: 0.5910 - val_accuracy: 0.7500\n",
            "Epoch 176/200\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.5748 - accuracy: 0.6938 - val_loss: 0.5903 - val_accuracy: 0.7375\n",
            "Epoch 177/200\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.5751 - accuracy: 0.7000 - val_loss: 0.5906 - val_accuracy: 0.7250\n",
            "Epoch 178/200\n",
            "32/32 [==============================] - 0s 1ms/step - loss: 0.5773 - accuracy: 0.7063 - val_loss: 0.5905 - val_accuracy: 0.7500\n",
            "Epoch 179/200\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.5746 - accuracy: 0.7000 - val_loss: 0.5889 - val_accuracy: 0.7625\n",
            "Epoch 180/200\n",
            "32/32 [==============================] - 0s 1ms/step - loss: 0.5760 - accuracy: 0.6906 - val_loss: 0.5946 - val_accuracy: 0.7250\n",
            "Epoch 181/200\n",
            "32/32 [==============================] - 0s 1ms/step - loss: 0.5757 - accuracy: 0.6969 - val_loss: 0.5895 - val_accuracy: 0.7625\n",
            "Epoch 182/200\n",
            "32/32 [==============================] - 0s 1ms/step - loss: 0.5755 - accuracy: 0.7031 - val_loss: 0.5929 - val_accuracy: 0.7500\n",
            "Epoch 183/200\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.5770 - accuracy: 0.7063 - val_loss: 0.5952 - val_accuracy: 0.7250\n",
            "Epoch 184/200\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.5729 - accuracy: 0.7000 - val_loss: 0.5892 - val_accuracy: 0.7625\n",
            "Epoch 185/200\n",
            "32/32 [==============================] - 0s 1ms/step - loss: 0.5783 - accuracy: 0.7000 - val_loss: 0.5898 - val_accuracy: 0.7625\n",
            "Epoch 186/200\n",
            "32/32 [==============================] - 0s 1ms/step - loss: 0.5758 - accuracy: 0.7031 - val_loss: 0.5926 - val_accuracy: 0.7250\n",
            "Epoch 187/200\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.5753 - accuracy: 0.7156 - val_loss: 0.5893 - val_accuracy: 0.7250\n",
            "Epoch 188/200\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.5742 - accuracy: 0.7063 - val_loss: 0.5896 - val_accuracy: 0.7500\n",
            "Epoch 189/200\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.5755 - accuracy: 0.6969 - val_loss: 0.5902 - val_accuracy: 0.7250\n",
            "Epoch 190/200\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.5761 - accuracy: 0.6969 - val_loss: 0.5900 - val_accuracy: 0.7250\n",
            "Epoch 191/200\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.5762 - accuracy: 0.7031 - val_loss: 0.5911 - val_accuracy: 0.7250\n",
            "Epoch 192/200\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.5794 - accuracy: 0.7063 - val_loss: 0.5898 - val_accuracy: 0.7250\n",
            "Epoch 193/200\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.5748 - accuracy: 0.7031 - val_loss: 0.5870 - val_accuracy: 0.7625\n",
            "Epoch 194/200\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.5752 - accuracy: 0.7031 - val_loss: 0.5911 - val_accuracy: 0.7250\n",
            "Epoch 195/200\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.5750 - accuracy: 0.7031 - val_loss: 0.5908 - val_accuracy: 0.7500\n",
            "Epoch 196/200\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.5744 - accuracy: 0.6938 - val_loss: 0.5910 - val_accuracy: 0.7625\n",
            "Epoch 197/200\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.5770 - accuracy: 0.7125 - val_loss: 0.5887 - val_accuracy: 0.7250\n",
            "Epoch 198/200\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.5737 - accuracy: 0.7063 - val_loss: 0.5867 - val_accuracy: 0.7625\n",
            "Epoch 199/200\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.5743 - accuracy: 0.6969 - val_loss: 0.5907 - val_accuracy: 0.7500\n",
            "Epoch 200/200\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.5729 - accuracy: 0.7094 - val_loss: 0.5911 - val_accuracy: 0.7250\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GgJEIXc7sbOv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "9c73e024-2558-4acf-beea-8c053ac94f50"
      },
      "source": [
        "np.set_printoptions(threshold=np.inf)\n",
        "print(model.trainable_variables)\n",
        "file = open('./weights.txt', 'w')\n",
        "for v in model.trainable_variables:\n",
        "    file.write(str(v.name) + '\\n')\n",
        "    file.write(str(v.shape) + '\\n')\n",
        "    file.write(str(v.numpy()) + '\\n')\n",
        "file.close()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[<tf.Variable 'dense/kernel:0' shape=(6, 6) dtype=float32, numpy=\n",
            "array([[-0.5419596 ,  0.5074848 , -0.5889225 ,  0.08272381, -0.06621957,\n",
            "         0.49242342],\n",
            "       [-0.3408262 ,  0.58436775, -0.15009545,  0.31645462, -0.48582566,\n",
            "        -0.5656811 ],\n",
            "       [-0.33331278, -0.13037899,  0.70403975, -0.6219486 ,  0.45213476,\n",
            "        -0.38537577],\n",
            "       [-0.16090876,  0.06327468, -0.09943253, -0.54789853,  0.09413129,\n",
            "         0.34802276],\n",
            "       [-0.12152874, -0.36982694,  0.5868229 ,  0.61531   , -0.00787216,\n",
            "        -0.6046874 ],\n",
            "       [-0.02682245,  0.24450172,  0.12497485,  0.04669806, -0.6419516 ,\n",
            "         0.8961478 ]], dtype=float32)>, <tf.Variable 'dense/bias:0' shape=(6,) dtype=float32, numpy=\n",
            "array([ 0.        , -0.09246845,  0.02128557,  0.02190281,  0.03909958,\n",
            "        0.14161906], dtype=float32)>, <tf.Variable 'dense_1/kernel:0' shape=(6, 64) dtype=float32, numpy=\n",
            "array([[-0.204686  ,  0.01122636,  0.06372717, -0.12557478,  0.05348223,\n",
            "         0.09848714,  0.01102716, -0.12977113,  0.07400528, -0.13197945,\n",
            "         0.1273525 , -0.13072316, -0.19530939, -0.2623692 ,  0.0980899 ,\n",
            "        -0.20174211,  0.13446084, -0.21389362,  0.20863283,  0.1817171 ,\n",
            "        -0.12766983,  0.01031867, -0.11963981, -0.1504962 ,  0.2203933 ,\n",
            "        -0.11700292,  0.22763968, -0.27357182,  0.22531116,  0.1257292 ,\n",
            "         0.23911744, -0.08212142, -0.06130268, -0.14765094,  0.15585753,\n",
            "         0.21746927, -0.11163215, -0.16190775,  0.28545552, -0.20837621,\n",
            "         0.28299087, -0.01712671,  0.17365023, -0.05489843,  0.0457308 ,\n",
            "        -0.20334533,  0.12327048, -0.06978472,  0.1671597 , -0.22175491,\n",
            "         0.09379983,  0.08056736,  0.2790696 ,  0.2902233 ,  0.22627181,\n",
            "         0.28517163,  0.13565871,  0.07349315, -0.25190148,  0.21666771,\n",
            "        -0.16051312, -0.10623577,  0.0935736 ,  0.05099526],\n",
            "       [-0.2785495 ,  0.01016594, -0.00975268, -0.0242888 , -0.05888411,\n",
            "        -0.14283295, -0.2301705 , -0.16619202, -0.24685724,  0.12987675,\n",
            "         0.19822916, -0.33583817,  0.04074392, -0.20677929, -0.1585906 ,\n",
            "        -0.06527209,  0.01610589, -0.34030718, -0.1507119 , -0.20780335,\n",
            "        -0.30029193, -0.22278434,  0.12146117, -0.20930819, -0.25202084,\n",
            "        -0.22568211,  0.10809745,  0.04674004,  0.21770158,  0.37718576,\n",
            "        -0.20227128, -0.2013168 , -0.03314703,  0.2898689 , -0.009602  ,\n",
            "        -0.13080947, -0.05357933, -0.10932212, -0.13237678, -0.03844847,\n",
            "        -0.05853223, -0.08350058,  0.09948301,  0.11741888,  0.16045532,\n",
            "        -0.57134104, -0.14100441,  0.2034067 , -0.19297835, -0.2751397 ,\n",
            "         0.28702888, -0.19766808, -0.0493094 , -0.31451377, -0.0583867 ,\n",
            "        -0.01204259, -0.20999241,  0.2054434 ,  0.30815834,  0.34289077,\n",
            "         0.28469795, -0.22577383, -0.04555539,  0.282727  ],\n",
            "       [ 0.3612119 ,  0.08752739, -0.53396785, -0.27435797, -0.22521919,\n",
            "         0.09470998,  0.4687563 ,  0.42306498,  0.37441146, -0.3728409 ,\n",
            "         0.13743828,  0.42514315, -0.23153181, -0.01255205,  0.35404924,\n",
            "        -0.3902241 , -0.03143749,  0.4405986 , -0.26982528, -0.1459699 ,\n",
            "         0.20657025,  0.38618246, -0.49263477,  0.32667074,  0.09581661,\n",
            "         0.344779  ,  0.4064045 ,  0.0602973 , -0.14975001, -0.02348898,\n",
            "        -0.07936949,  0.3871022 , -0.05580039, -0.1269106 , -0.50575805,\n",
            "        -0.18654107,  0.2765414 , -0.142576  ,  0.06259449, -0.48001066,\n",
            "        -0.2372595 , -0.2449517 ,  0.24212427,  0.06947976,  0.35145557,\n",
            "         0.58180493, -0.14958969,  0.04123637, -0.2903322 , -0.23831114,\n",
            "         0.1665743 ,  0.3213919 ,  0.1609832 ,  0.4252832 , -0.17587271,\n",
            "        -0.03443561, -0.26549196,  0.20880288,  0.01628021, -0.13610403,\n",
            "        -0.03879947,  0.06141835,  0.277994  , -0.14623386],\n",
            "       [ 0.21856266,  0.04734658, -0.08006062, -0.24548817, -0.06829144,\n",
            "         0.29349774, -0.03518766,  0.02277685, -0.23258978,  0.14865635,\n",
            "         0.19680262,  0.14558876, -0.13100323, -0.22488476, -0.28395087,\n",
            "        -0.08500297,  0.23544164,  0.1140721 , -0.25979474, -0.14728734,\n",
            "        -0.12347731,  0.01736438, -0.24847664,  0.08426208, -0.18524319,\n",
            "        -0.25858688,  0.11037973,  0.30155534, -0.04761552, -0.01357642,\n",
            "        -0.0947362 , -0.05686511,  0.10661115,  0.06987018, -0.14276628,\n",
            "         0.16963053,  0.03161331, -0.28026542,  0.05998817, -0.12574728,\n",
            "        -0.23252632,  0.04121131,  0.30603883,  0.11617044, -0.265279  ,\n",
            "         0.15635501, -0.26984438,  0.07457791, -0.19815242, -0.22205499,\n",
            "        -0.21879557,  0.15288232,  0.18027349,  0.13490793, -0.09723595,\n",
            "         0.15216504, -0.24661657, -0.19943449,  0.13480736, -0.04720898,\n",
            "         0.21713811,  0.30391213,  0.09911355, -0.18918952],\n",
            "       [ 0.18817736,  0.24245997, -0.2988978 , -0.0438948 , -0.07285306,\n",
            "         0.39083964,  0.2785498 ,  0.20045196,  0.4221878 ,  0.9001623 ,\n",
            "        -0.89457375,  0.49791554,  1.0029498 , -0.03106466,  0.3834781 ,\n",
            "        -0.77823454,  0.26189682,  0.15327653, -0.13222474, -0.21431175,\n",
            "         0.10337498,  0.20738418, -0.21308354,  0.23234127, -0.01953214,\n",
            "         0.1825759 ,  0.36905685,  0.4115083 ,  0.20935449, -0.0907799 ,\n",
            "         0.28438157,  0.30996495,  0.46194825,  0.03532544, -0.5687148 ,\n",
            "         0.09634599, -0.03275801, -0.11111856,  0.3310303 , -0.42964953,\n",
            "         0.07640591, -0.19089526,  0.4502726 ,  0.25295135, -0.195301  ,\n",
            "        -0.04626136, -0.09831195, -0.40984347, -0.04632173, -0.12198453,\n",
            "        -0.10976549,  0.17882106,  0.17862184,  0.45903596, -0.05502373,\n",
            "         0.24247083,  0.2807805 , -0.08477033,  0.04568905, -0.0563293 ,\n",
            "         0.01043006,  0.05509365,  0.41024113, -0.03406508],\n",
            "       [-0.27276784,  0.08091232,  0.13317513, -0.06406397, -0.19403662,\n",
            "         0.28092936,  0.38909242,  0.36094624, -0.29045108, -0.05892516,\n",
            "        -0.11010633, -0.07035758, -0.01907031,  0.19823512, -0.25802416,\n",
            "         0.23359968,  0.09128276,  0.2365573 ,  0.02752557, -0.2436225 ,\n",
            "        -0.06293882, -0.04635797,  0.17181575, -0.0575821 ,  0.06399199,\n",
            "        -0.27383432,  0.26263884,  0.04904176,  0.06609942, -0.3250205 ,\n",
            "        -0.28424186,  0.33739915,  0.3083416 ,  0.04031879,  0.26621935,\n",
            "         0.12290944,  0.14451042,  0.13174285,  0.08976865,  0.32769185,\n",
            "         0.04754317, -0.09211996,  0.25719115, -0.11635441, -0.13104339,\n",
            "         0.225855  ,  0.06816748, -0.17585945,  0.18333203, -0.18955317,\n",
            "        -0.25793314, -0.22220248,  0.2539542 , -0.10747448,  0.06682846,\n",
            "        -0.06391984,  0.03692815, -0.12770586, -0.17227444, -0.36613256,\n",
            "        -0.18806542,  0.26328424,  0.20389174, -0.2354588 ]],\n",
            "      dtype=float32)>, <tf.Variable 'dense_1/bias:0' shape=(64,) dtype=float32, numpy=\n",
            "array([ 0.07190645, -0.09094295,  0.00914817,  0.        ,  0.        ,\n",
            "        0.06698319,  0.1469427 ,  0.12293907,  0.07520895, -0.06829412,\n",
            "       -0.09755849,  0.14360291, -0.0434343 ,  0.        , -0.00777223,\n",
            "        0.06881601,  0.08117989,  0.11386969,  0.        ,  0.        ,\n",
            "       -0.02783381,  0.02921503,  0.02402493,  0.04494039,  0.        ,\n",
            "        0.0310493 ,  0.08311939,  0.0860308 , -0.17348166, -0.03159287,\n",
            "        0.        ,  0.07561255,  0.04553349, -0.0755146 ,  0.10195813,\n",
            "       -0.08621231,  0.08719604, -0.01382905,  0.10400689,  0.12664385,\n",
            "       -0.01799865, -0.01276473,  0.08053944,  0.07356947,  0.03045271,\n",
            "       -0.0253793 ,  0.        , -0.04487868,  0.        ,  0.        ,\n",
            "        0.06084305,  0.0346948 ,  0.06430306,  0.13068809,  0.        ,\n",
            "        0.06665   ,  0.        , -0.0496902 , -0.10802922,  0.04684752,\n",
            "       -0.10710271,  0.13718903, -0.02892519,  0.03338575], dtype=float32)>, <tf.Variable 'dense_2/kernel:0' shape=(64, 2) dtype=float32, numpy=\n",
            "array([[ 0.32897383, -0.01250001],\n",
            "       [-0.24606076, -0.23939668],\n",
            "       [ 0.30595812,  0.19723715],\n",
            "       [ 0.27589273,  0.02650037],\n",
            "       [-0.03026369,  0.28073907],\n",
            "       [-0.0466943 , -0.3636826 ],\n",
            "       [ 0.66142714, -0.16074479],\n",
            "       [ 0.19312888, -0.44872534],\n",
            "       [ 0.68386173, -0.45426092],\n",
            "       [-0.40674967,  0.33005467],\n",
            "       [-0.31170648,  0.12589775],\n",
            "       [ 0.47151765, -0.28036502],\n",
            "       [-0.99290097,  1.0497569 ],\n",
            "       [-0.29486254,  0.04754758],\n",
            "       [ 0.5142175 , -0.32842094],\n",
            "       [ 0.04013671, -0.15279691],\n",
            "       [ 0.14304182, -0.17721112],\n",
            "       [ 0.15982896, -0.554495  ],\n",
            "       [-0.08198257,  0.03373456],\n",
            "       [-0.21525207,  0.1992063 ],\n",
            "       [-0.18845847,  0.05627018],\n",
            "       [ 0.9106473 , -0.4321247 ],\n",
            "       [ 0.3027519 , -0.03361783],\n",
            "       [ 0.06323908, -0.3798835 ],\n",
            "       [-0.26402205, -0.21668296],\n",
            "       [ 0.53468156, -0.7930069 ],\n",
            "       [ 0.05998753, -0.29638335],\n",
            "       [ 0.29435802, -0.23537523],\n",
            "       [-0.5283908 ,  0.04190104],\n",
            "       [-0.26449347,  0.4203182 ],\n",
            "       [-0.2983718 , -0.17384896],\n",
            "       [ 0.3297459 , -0.43558896],\n",
            "       [ 0.29140666,  0.21918932],\n",
            "       [-0.17270826,  0.17458324],\n",
            "       [ 0.27226767,  0.04031699],\n",
            "       [ 0.11855095,  0.17503841],\n",
            "       [ 0.29051086, -0.2211686 ],\n",
            "       [-0.1329806 , -0.13421887],\n",
            "       [ 0.06657191, -0.19738394],\n",
            "       [ 0.40736842, -0.24083833],\n",
            "       [-0.11781404, -0.08748098],\n",
            "       [ 0.00713382,  0.27259848],\n",
            "       [ 0.2934617 ,  0.0091101 ],\n",
            "       [ 0.07220411, -0.14626805],\n",
            "       [-0.3257156 , -0.04738973],\n",
            "       [ 0.58710414, -0.760563  ],\n",
            "       [ 0.20003438,  0.03188556],\n",
            "       [-0.2972262 ,  0.26085705],\n",
            "       [ 0.02529916,  0.1072765 ],\n",
            "       [-0.0013079 , -0.07818297],\n",
            "       [-0.40339246,  0.15686223],\n",
            "       [ 0.29203475,  0.03374593],\n",
            "       [-0.00983442, -0.1281431 ],\n",
            "       [ 0.20726182, -0.50623554],\n",
            "       [-0.24542496, -0.07732128],\n",
            "       [ 0.02111593, -0.11050452],\n",
            "       [ 0.19567639,  0.05593067],\n",
            "       [-0.3637549 , -0.01180115],\n",
            "       [-0.20402436,  0.28183922],\n",
            "       [-0.12801687,  0.5217271 ],\n",
            "       [-0.16125068,  0.06996035],\n",
            "       [ 0.41971847, -0.26029533],\n",
            "       [-0.0282132 , -0.05730195],\n",
            "       [-0.35259932,  0.5118556 ]], dtype=float32)>, <tf.Variable 'dense_2/bias:0' shape=(2,) dtype=float32, numpy=array([ 0.09341932, -0.09341934], dtype=float32)>]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}